#!/bin/bash
#
# testbed/cluster discovery and validation
# 	1. Discover cluster_type, num_workers and num_cpus (per worker)
# 	2. Confim the user-intent workers specified in lab.config.
#
auto_fix="true"


source ${REG_ROOT}/lab.config

# We compute the number of CPUs for each pod based on the node.allocatable.cpu.
# We learn this value during during lab-analyzer time, and this value can go down
# when we install additional k8s services/features i,.e SRIOV and PAO. Hence
# we "reserve" some CPUs for it one time. We have seen 4 was not enough sometime.
# Hence use 8.
RESERVED_CPUS=10

# Define the JSON content
tb_info='{
    "CLUSTER_TYPE":			"value1",
    "NUM_WORKERS": 			"value3",
    "ALLOCATABLE_CPUS": 	"value3",
    "USEABLE_CPUS": 		"value3",
    "CORE_PERSOCKET": 		"value3"
}'

function f_verify_lab_config {
    # Verify a few common missed setup activities.
    #   REG_KNI_USER (root vs kni)
    #   All export vars (most likely b/c user forgot to fixup lab.config)
    #   SRIOV-config (user has not invoked "make init") - we can do it, but let the user do it.
    #   PAO-config (user has not invoked "make init")

    local DEST="$REG_KNI_USER@$REG_OCPHOST"
    workers=$(ssh $DEST "oc get nodes --selector='node-role.kubernetes.io/worker' -o jsonpath='{.items[*].metadata.name}'")
    if [ $? -ne 0 ] ; then
        echo "Cannot ssh to bastion $DEST" >&2
        exit 1
    fi
    if [[ !  "$workers" == *"$OCP_WORKER_0"* ]]; then
        echo "OCP_WORKER_0=$OCP_WORKER_0 is NOT in '$workers'"  >&2
        echo "Check lab.config on both sides"  >&2
        exit 1
    fi
    if [[ !  "$workers" == *"$OCP_WORKER_1"* ]]; then
        echo "OCP_WORKER_0=$OCP_WORKER_1 is NOT in '$workers'"  >&2
        echo "Check lab.config on both sides"  >&2
        exit
    fi
    if [[ !  "$workers" == *"$OCP_WORKER_2"* ]]; then
        echo "OCP_WORKER_0=$OCP_WORKER_2 is NOT in '$workers'"  >&2
        echo "Check lab.config on both sides"  >&2
        exit 1
    fi

    # Check lab.config consistency
    reg_dir=$(basename "$REG_ROOT")
    rem_exports=$(ssh $DEST "cd $reg_dir && cat lab.config | grep export | grep -v '#' ")
    local_exports=$(cd $REG_ROOT && cat lab.config | grep export | grep -v '#')
    if [ "$rem_exports" != "$local_exports" ]; then
        if [ "$auto_fix" == "true" ]; then
            echo "WARN: $reg_dir/lab.config file on controller differs the one on the bastion" >&2
            echo "auto_fix is ON. Fixing it"  >&2
            scp $REG_ROOT/lab.config  $DEST:$REG_ROOT/lab.config
        else
            echo "ERROR: $reg_dir/lab.config file on controller differs the one on the bastion" >&2
        fi
    fi

    # A super convolute mess. We need to make init-lab (to initialize CLUSTER_TYPE) on the bastion 
    # first before we can run make init for SRIOV/UNIV/
    if [ "$auto_fix" == "true" ]; then
        echo "CMD: ssh $DEST cd $reg_dir && source bootstrap.sh && make init-lab" >&2
        ssh $DEST "cd $reg_dir && source bootstrap.sh && make init-lab"
    fi

    # Check SRIOV-config and PAO-config ready on the bastion
    file=$(ssh $DEST "cd $reg_dir && source bootstrap.sh && cd SRIOV-config/UNIV && ls setting.env")
    if [ -z "$file" ] ; then
        if [ "$auto_fix" == "true" ]; then
            echo "WARN: SRIOV-config/UNIV is not yet initialized on the bastion" >&2
            echo "auto_fix is ON. Fixing it"  >&2
            (ssh $DEST "cd $reg_dir && source bootstrap.sh && cd SRIOV-config/UNIV && make init")
        else
            echo "ERROR: SRIOV-config/UNIV is not yet initialized on the bastion" >&2
            echo "Hint: go to $reg_dir/SRIOV-config/UNIV on the bastion and 'make init'" >&2
            exit 1
        fi
    fi

    file=$(ssh $DEST "cd $reg_dir && source bootstrap.sh && cd PAO-config && ls setting.env")
    if [ -z "$file" ] ; then
        if [ "$auto_fix" == "true" ]; then
            echo "WARN: PAO-config is not yet initialized on the bastion" >&2
            echo "auto_fix is ON. Fixing it"  >&2
            (ssh $DEST "cd $reg_dir && source bootstrap.sh && cd PAO-config/ && make init")

        else
            echo "ERROR: PAO-config is not yet initialized on the bastion" >&2
            echo "Hint: go to $reg_dir/PAO-config on the bastion and 'make init'" >&2
            exit 1
        fi
    fi

    echo "Lab config check done " >&2
    return
}

function f_update_tb_info {
	# Read the JSON file
	json=$tb_info
	updated_json=$(echo "$json" | jq ".CLUSTER_TYPE = \"$tb_cluster_type\" | .NUM_WORKERS = \"$tb_num_workers\" \
								|     .ALLOCATABLE_CPUS = \"$tb_alloc_cpus\" |  .USEABLE_CPUS = \"$tb_useable_cpus\" \
                                |     .CORE_PERSOCKET = \"$tb_cps\"")
	# Write the JSON content to a file
	echo "$updated_json" 
}

function f_get_cluster_type {
	# number of node is 1 or 3 or more
	num_node="$(ssh $REG_KNI_USER@$REG_OCPHOST "kubectl get nodes --no-headers" | wc -l)"
	case $num_node in
		1)
			echo "SNO"
			;;
		3)
			echo "COMPACT"
			;;
		*)
			echo "STANDARD"
			;;
	esac
}

function f_validate_worker_nodes {
	workers="$(ssh $REG_KNI_USER@$REG_OCPHOST "kubectl get nodes --no-headers --selector=node-role.kubernetes.io/worker=")"
	tb_num_workers=$(echo "$workers" | wc -l)
	worker_names=$(echo "$workers" | awk '{print $1}')
	#echo names=$worker_names
	# verify workers in lab.conf
	if [[ "$worker_names" != *"$OCP_WORKER_0"* ]]; then
		echo "Please correct OCP_WORKER_0=\"$OCP_WORKER_0\" var in lab.config. Worker nodes are $worker_names" >&2
		exit 1
	fi
	if [[ "$worker_names" != *"$OCP_WORKER_1"* ]]; then
		echo "Please correct OCP_WORKER_1=\"$OCP_WORKER_1\" var in lab.config. Worker nodes are $worker_names" >&2
		exit 1
	fi
	if [[ "$worker_names" != *"$OCP_WORKER_2"* ]]; then
		echo "Please correct OCP_WORKER_2=\"$OCP_WORKER_2\" var in lab.config. Worker nodes are $worker_names" >&2
		exit 1
	fi
}

function create_mirror_repo_on_bastion {
    local_repo_path=$(git rev-parse --show-toplevel)
    remote_user="$REG_KNI_USER"
    remote_host="$REG_OCPHOST"

    #
    # Handle a complication when dealing with different "dirname" on the controller 
    # and on the bastion .ie /root/my-regulus versus /home/user/my-regulus
    #
    reg_dir=$(basename "$REG_ROOT")

    # Set the remote repository path to match the local path

    local_basename=$(basename "$REG_ROOT")
    remote_dirname=$(ssh ${remote_user}@${remote_host} "pwd")
    remote_repo_path=$remote_dirname/$local_basename

    # Get the current branch name from the local repository
    current_branch=$(git branch --show-current)

    # Check if the remote repository already exists
    repo_exists=$(ssh ${remote_user}@${remote_host} "[ -d ${remote_repo_path}/.git ] && echo 'exists' || echo 'not exists'")

    if [ "$repo_exists" = "exists" ]; then
        echo "The repository already exists on the bastion host at ${remote_repo_path}. No action taken." >&2
    else
        echo "The repository does not exist. Creating the repository on the bastion host..."  >&2
 
        # Create the regulus dir on the bastion host
        ssh ${remote_user}@${remote_host} "mkdir -p ${remote_repo_path}"

        # Initialize a non-bare repository on the bastion host
        ssh ${remote_user}@${remote_host} "cd ${remote_repo_path} && git init"

        # Add the remote repository to local Git config. But remove remote first if there was a old one.
        git remote remove peer-regulus  2>/dev/null 
        echo "CMD: git remote add peer-regulus ${remote_user}@${remote_host}:${remote_repo_path}" >&2
        git remote add peer-regulus ${remote_user}@${remote_host}:${remote_repo_path} 

        # Push your local repository to the remote/bastion repository
        echo "CMD: git push peer-regulus ${current_branch}"  >&2
        git push peer-regulus ${current_branch}

        # Switch to the current branch on the remote/bastion machine
        echo "CMD: ssh ${remote_user}@${remote_host} cd ${remote_repo_path} && git checkout ${current_branch}"  >&2
        ssh ${remote_user}@${remote_host} "cd ${remote_repo_path} && git checkout ${current_branch}"

        echo "Repository has been successfully created and mirrored to the bastion host. The branch '${current_branch}' has been checked out."  >&2
    fi
}


# Install regulus repo on the bastion if applicable
create_mirror_repo_on_bastion

# Let's go 
if [ "$(hostname)" != "$REG_OCPHOST" ]  &&  [ "$(hostname -i)" != "$REG_OCPHOST" ] ; then
    echo "$0 is running on the controller ... check lab.config match" >&2
    f_verify_lab_config 
else
    echo "$0 is running on the bastion ... skip lab.config match check" >&2
fi

if ! ssh $REG_KNI_USER@$REG_OCPHOST oc get node  &>/dev/null ; then
	echo "Error: Please confirm REG_KNI_USER and REG_OCPHOST in lab.config" >&2
	exit 1
fi


tb_cluster_type=$(f_get_cluster_type)
echo "cluster_type=$tb_cluster_type" >&2
f_validate_worker_nodes
first_worker="$(echo $worker_names | awk '{print $1}')"
tb_alloc_cpus="$(ssh $REG_KNI_USER@$REG_OCPHOST "kubectl get node/$first_worker -ojsonpath='{.status.allocatable.cpu}'")"
# Round-down to whole CPU if we see like 31500m
if [[ "$tb_alloc_cpus" == *m ]]; then
    tb_alloc_cpus="${tb_alloc_cpus:0: -4}"
fi

tb_useable_cpus=$(($tb_alloc_cpus-$RESERVED_CPUS))
echo "useable_cpus=$tb_useable_cpus" >&2

# get worker cpu topology
cps_string="$(ssh $REG_KNI_USER@$REG_OCPHOST "ssh core@$first_worker lscpu" | grep Core)"
if [ -z "$cps_string" ]; then
    echo "Failed to ssh core@$first_worker. Please setup 'core' login" >&2
    exit 
fi
# extract the number from like "Core(s) per socket:  28"
tb_cps=$(echo "$cps_string" | awk '{print $NF}')
echo "core-per-socket=$tb_cps" >&2 

f_update_tb_info

echo "$0 completed" >&2

# done
