#!/usr/bin/env bash
#
# testbed/cluster discovery and validation
# 	1. Discover cluster_type, num_workers and num_cpus (per worker)
# 	2. Confim the user-intent workers specified in lab.config.
#
auto_fix="true"

source ${REG_ROOT}/lab.config

# About $reg_dir,
# On controller, the user can create REGULUS somewhere under their home i.e /root/path/regulus
# On bastion, the user can be root or kni. Since regulus will clone a repo there. it wil be /home/kni/path/regulus or /root/path/regulus.
# We need to identify the relative path from the bastion user's HOME. That is the "path/regulus" so that we can ssh to the bastion and cd to "path/regulus"
reg_dir="${PWD#"$HOME"/}"
echo reg_dir=$reg_dir >&2

# We compute the number of CPUs for each pod based on the node.allocatable.cpu.
# We learn this value during during lab-analyzer time, and this value can go down
# when we install additional k8s services/features i,.e SRIOV and PAO. Hence
# we "reserve" some CPUs for it one time. We have seen 4 was not enough sometime.
# Hence use 8.
RESERVED_CPUS=10

# Define the JSON content
tb_info='{
    "CLUSTER_TYPE":			"value1",
    "NUM_WORKERS": 			"value3",
    "ALLOCATABLE_CPUS": 	"value3",
    "USEABLE_CPUS": 		"value3",
    "CORE_PERSOCKET": 		"value3"
}'

function exit_error() {
    local msg
    msg="$1"
    echo "[ERROR] ${msg}" 1>&2
    exit 1
}


function do_ssh() {
    local user_host user host ssh_cmd
    user_host=$1; shift
    user=`echo $user_host | awk -F@ '{print $1}'`
    host=`echo $user_host | awk -F@ '{print $2}'`
    ssh_cmd=""
    if [ -z "$user" ]; then
        exit_error "do_ssh: user was blank: $user_host"
    fi
    if [ -z "$host" ]; then
        exit_error "do_ssh: host was blank: $user_host"
    fi
    ssh_cmd="ssh $ssh_opts $user_host bash -c \"$@\""

    echo CMD: $ssh_cmd  >&2
    $ssh_cmd
    local rc=$?
    return $rc
}


function f_verify_lab_config {
echo IN: f_verify_lab_config  >&2
    # Verify a few common missed setup activities.
    #   REG_KNI_USER (root vs kni)
    #   All export vars (most likely b/c user forgot to fixup lab.config)
    #   SRIOV-config (user has not invoked "make init") - we can do it, but let the user do it.
    #   PAO-config (user has not invoked "make init")

    local DEST="$REG_KNI_USER@$REG_OCPHOST"
    #workers=$(ssh $DEST "oc get nodes --selector='node-role.kubernetes.io/worker' -o jsonpath='{.items[*].metadata.name}'")
    workers=$(do_ssh $DEST "source $reg_dir/lab.config &&  oc get nodes --request-timeout=30s --selector='node-role.kubernetes.io/worker' -o jsonpath='{.items[*].metadata.name}'")
    
    if [ $? -ne 0 ] ; then
        echo "Cannot ssh to bastion $DEST" >&2
        exit 1
    fi
    if [[ !  "$workers" == *"$OCP_WORKER_0"* ]]; then
        echo "OCP_WORKER_0=$OCP_WORKER_0 is NOT in '$workers'"  >&2
        echo "Check lab.config on both sides"  >&2
        exit 1
    fi
    if [[ !  "$workers" == *"$OCP_WORKER_1"* ]]; then
        echo "OCP_WORKER_0=$OCP_WORKER_1 is NOT in '$workers'"  >&2
        echo "Check lab.config on both sides"  >&2
        exit
    fi
    if [[ !  "$workers" == *"$OCP_WORKER_2"* ]]; then
        echo "OCP_WORKER_0=$OCP_WORKER_2 is NOT in '$workers'"  >&2
        echo "Check lab.config on both sides"  >&2
        exit 1
    fi

    # Check lab.config consistency
    #rem_exports=$(ssh $DEST "cd $reg_dir && cat lab.config | grep export | grep -v '#' ")
    rem_exports=$(do_ssh $DEST "cd $reg_dir && cat lab.config | grep export | grep -v '#' ")
    local_exports=$(cd $REG_ROOT && cat lab.config | grep export | grep -v '#')
    if [ "$rem_exports" != "$local_exports" ]; then
        if [ "$auto_fix" == "true" ]; then
            echo "WARN: $reg_dir/lab.config file on controller differs the one on the bastion" >&2
            echo "auto_fix is ON. Fixing it"  >&2
            scp $REG_ROOT/lab.config  $DEST:$REG_ROOT/lab.config
        else
            echo "ERROR: $reg_dir/lab.config file on controller differs the one on the bastion" >&2
        fi
    fi

    # A super convolute mess. We need to make init-lab (to initialize CLUSTER_TYPE) on the bastion 
    # first before we can run make init for SRIOV/UNIV/
    if [ "$auto_fix" == "true" ]; then
        echo "CMD: ssh $DEST cd $reg_dir && source bootstrap.sh && make init-lab" >&2
        #ssh $DEST "cd $reg_dir && source bootstrap.sh && make init-lab"
        do_ssh $DEST "cd $reg_dir && source bootstrap.sh && make init-lab"
    fi

    # Check SRIOV-config and PAO-config ready on the bastion
    file=$(do_ssh $DEST "cd $reg_dir && source bootstrap.sh && cd SRIOV-config/UNIV && ls setting.env")
    if [ -z "$file" ] ; then
        if [ "$auto_fix" == "true" ]; then
            echo "WARN: SRIOV-config/UNIV is not yet initialized on the bastion" >&2
            echo "auto_fix is ON. Fixing it"  >&2
            (do_ssh $DEST "cd $reg_dir && source bootstrap.sh && cd SRIOV-config/UNIV && make init")
        else
            echo "ERROR: SRIOV-config/UNIV is not yet initialized on the bastion" >&2
            echo "Hint: go to $reg_dir/SRIOV-config/UNIV on the bastion and 'make init'" >&2
            exit 1
        fi
    fi

    file=$(do_ssh $DEST "cd $reg_dir && source bootstrap.sh && cd PAO-config && ls setting.env")
    if [ -z "$file" ] ; then
        if [ "$auto_fix" == "true" ]; then
            echo "WARN: PAO-config is not yet initialized on the bastion" >&2
            echo "auto_fix is ON. Fixing it"  >&2
            (do_ssh $DEST "cd $reg_dir && source bootstrap.sh && cd PAO-config/ && make init")

        else
            echo "ERROR: PAO-config is not yet initialized on the bastion" >&2
            echo "Hint: go to $reg_dir/PAO-config on the bastion and 'make init'" >&2
            exit 1
        fi
    fi

    echo "Lab config check done " >&2
    return
}

function f_update_tb_info {
	# Read the JSON file
	json=$tb_info
	updated_json=$(echo "$json" | jq ".CLUSTER_TYPE = \"$tb_cluster_type\" | .NUM_WORKERS = \"$tb_num_workers\" \
								|     .ALLOCATABLE_CPUS = \"$tb_alloc_cpus\" |  .USEABLE_CPUS = \"$tb_useable_cpus\" \
                                |     .CORE_PERSOCKET = \"$tb_cps\"")
	# Write the JSON content to a file
	echo "$updated_json" 
}

function f_get_cluster_type {
	# number of node is 1 or 3 or more
	num_node="$(do_ssh $REG_KNI_USER@$REG_OCPHOST "source $reg_dir/lab.config && kubectl get nodes --no-headers" | wc -l)"
	case $num_node in
		1)
			echo "SNO"
			;;
		3)
			echo "COMPACT"
			;;
		*)
			echo "STANDARD"
			;;
	esac
}

function f_validate_worker_nodes {
	workers="$(do_ssh $REG_KNI_USER@$REG_OCPHOST "source $reg_dir/lab.config && kubectl get nodes --no-headers --selector=node-role.kubernetes.io/worker=")"
	tb_num_workers=$(echo "$workers" | wc -l)
	worker_names=$(echo "$workers" | awk '{print $1}')
	#echo names=$worker_names
	# verify workers in lab.conf
	if [[ "$worker_names" != *"$OCP_WORKER_0"* ]]; then
		echo "Please correct OCP_WORKER_0=\"$OCP_WORKER_0\" var in lab.config. Worker nodes are $worker_names" >&2
		exit 1
	fi
	if [[ "$worker_names" != *"$OCP_WORKER_1"* ]]; then
		echo "Please correct OCP_WORKER_1=\"$OCP_WORKER_1\" var in lab.config. Worker nodes are $worker_names" >&2
		exit 1
	fi
	if [[ "$worker_names" != *"$OCP_WORKER_2"* ]]; then
		echo "Please correct OCP_WORKER_2=\"$OCP_WORKER_2\" var in lab.config. Worker nodes are $worker_names" >&2
		exit 1
	fi
}

function create_mirror_repo_on_bastion {
    local_repo_path=$(git rev-parse --show-toplevel)
    remote_user=$1
    remote_host=$2

    local_basename=$(basename "$REG_ROOT")

    # Get the current branch name from the local repository
    current_branch=$(git branch --show-current)

    # Check if the remote repository already exists
    repo_exists=$(do_ssh ${remote_user}@${remote_host} "[ -d ${reg_dir}/.git ] && echo 'exists' || echo 'not exists'")

    if [ "$repo_exists" = "exists" ]; then
        echo "The repository already exists on the bastion host at ${reg_dir}. No action taken." >&2
        do_ssh ${remote_user}@${remote_host} "rm -fr ${reg_dir}"
    fi
    #else
        echo "The repository does not exist. Creating the repository on the bastion host..."  >&2
 
        # Create the regulus dir on the bastion host
        do_ssh ${remote_user}@${remote_host} "mkdir -p ${reg_dir}"

        # Initialize a non-bare repository on the bastion host
        
        do_ssh ${remote_user}@${remote_host} "cd ${reg_dir} && git init 2>/dev/null"

        # Add the remote repository to local Git config. But remove remote first if there was a old one.
        git remote remove $local_basename-peer-regulus  2>/dev/null 
        echo "CMD: git remote add $local_basename-peer-regulus ${remote_user}@${remote_host}:${reg_dir}" >&2


        git remote add $local_basename-peer-regulus ${remote_user}@${remote_host}:${reg_dir} 


        # Push your local repository to the remote/bastion repository
        echo "CMD: git push $local_basename-peer-regulus ${current_branch}"  >&2
        git push $local_basename-peer-regulus ${current_branch}

        # Switch to the current branch on the remote/bastion machine
        echo "CMD: ssh ${remote_user}@${remote_host} cd ${reg_dir} && git checkout ${current_branch}"  >&2
        do_ssh ${remote_user}@${remote_host} "cd ${reg_dir} && git checkout ${current_branch}"

        # finally copy mirror lab.config and jobs.config to remote
        scp lab.config ${remote_user}@${remote_host}:~/${reg_dir}/
        scp jobs.config ${remote_user}@${remote_host}:~/${reg_dir}/
        scp bin/lab-analyzer ${remote_user}@${remote_host}:~/${reg_dir}/bin/

        echo "Repository has been successfully created and mirrored to the bastion host. The branch '${current_branch}' has been checked out."  >&2
    #fi
}

# continue
if [ "$(hostname)" != "$REG_OCPHOST" ]  &&  [ "$(hostname -i)" != "$REG_OCPHOST" ] ; then
    echo "$0 is running on the controller ... check lab.config match" >&2
    # mirror repo in a sub-shell, to avoid all the unwanted messages
    (
        # Install regulus repo on the bastion if applicable
        create_mirror_repo_on_bastion "${REG_KNI_USER}" "${REG_OCPHOST}"

    ) > lab-init.log 2>&1
    f_verify_lab_config 
else
    echo "$0 is running on the bastion ... skip lab.config match check" >&2
fi
 
if [[ -n "${BM_HOSTS[*]}" ]]; then
    DPDK_REMHOST="${BM_HOSTS[0]}"
    if [ "$(hostname)" != "$REG_OCPHOST" ]  &&  [ "$(hostname -i)" != "$REG_OCPHOST" ] ; then
      echo "$0 is running on the controller ... check lab.config match" >&2
      # mirror repo in a sub-shell, to avoid all the unwanted messages
      (
        # Install regulus repo on the DPDK host if applicable
          create_mirror_repo_on_bastion "root"  "${DPDK_REMHOST}"
     ) > rem-init.log 2>&1
    else
      echo "$0 is running on the bastion ... skip mirror to remhost" >&2
    fi
fi


echo CMD: reg_dir=$reg_dir  >&2
echo CMD: do_ssh $REG_KNI_USER@$REG_OCPHOST "source $reg_dir/lab.config && oc get node"  >&2

if ! do_ssh $REG_KNI_USER@$REG_OCPHOST "source $reg_dir/lab.config && oc get node"  &>/dev/null ; then
	echo "Error: Please confirm REG_KNI_USER and REG_OCPHOST in lab.config" >&2
	exit 1
fi


tb_cluster_type=$(f_get_cluster_type)
echo "cluster_type=$tb_cluster_type" >&2
f_validate_worker_nodes
first_worker="$(echo $worker_names | awk '{print $1}')"
tb_alloc_cpus="$(do_ssh $REG_KNI_USER@$REG_OCPHOST "source $reg_dir/lab.config && kubectl get node/$first_worker -ojsonpath='{.status.allocatable.cpu}'")"


# Round-down to whole CPU if we see like 31500m
if [[ "$tb_alloc_cpus" == *m ]]; then
    tb_alloc_cpus="${tb_alloc_cpus:0: -4}"
fi

tb_useable_cpus=$(($tb_alloc_cpus-$RESERVED_CPUS))
echo "useable_cpus=$tb_useable_cpus" >&2

# get worker cpu topology
cps_string="$(do_ssh $REG_KNI_USER@$REG_OCPHOST "ssh core@$first_worker lscpu" | grep Core)"
if [ -z "$cps_string" ]; then
    echo "Failed to ssh core@$first_worker. Please setup 'core' login" >&2
    exit 
fi
# extract the number from like "Core(s) per socket:  28"
tb_cps=$(echo "$cps_string" | awk '{print $NF}')
echo "core-per-socket=$tb_cps" >&2 


f_update_tb_info

echo "$0 completed" >&2

# done
